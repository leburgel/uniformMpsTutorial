{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do all necessary imports for this chapter\n",
    "using LinearAlgebra\n",
    "using TensorOperations\n",
    "using TensorKit\n",
    "using KrylovKit\n",
    "using OptimKit\n",
    "using Printf\n",
    "using Plots\n",
    "include(\"TutorialFunctions.jl\")\n",
    "using .TutorialFunctions: createMPS, normalizeMPS, fixedPoints, rightOrthonormalize, mixedCanonical, expVal2Uniform, expVal2Mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tangent-space methods for uniform matrix product states](https://doi.org/10.21468/SciPostPhysLectNotes.7)\n",
    "\n",
    "## 2. Finding ground states of local Hamiltonians\n",
    "\n",
    "In the previous chapter, we stated that uniform MPS can be used to efficiently approximate low-energy states of one-dimensional systems with gapped local Hamiltonians. Having defined ways of representing and manipulating MPS, the logical next step is therefore to have a look at how exactly they can be used to find ground states. To this end, we consider a nearest-neighbour Hamiltonian $H$  of the form\n",
    "\n",
    "$$H = \\sum_n h_{n, n+1}$$\n",
    "\n",
    "acting on an infinite one-dimensional system. Here, $h_{n,n+1}$ is a hermitian operator acting non-trivially on sites $n$ and $n+1$. As in any variational approach, the variational principle serves as a guide for finding ground-state approximations, dictating that the optimal MPS approximation of the ground state corresponds to the minimum of the expectation value of the energy,\n",
    "\n",
    "$$ \\min_A \\frac{\\left \\langle \\Psi(\\bar{A}) \\middle | H  \\middle | \\Psi(A) \\right \\rangle}{\\left \\langle \\Psi(\\bar{A}) \\middle | \\Psi(A)  \\right \\rangle}. $$\n",
    "\n",
    "In the thermodynamic limit the energy diverges with system size, but, since we are working with translation-invariant states only, we should rather minimize the energy density. In the following we will always restrict our discussion to properly normalized states. Diagrammatically, the minimization problem can then be recast as\n",
    "\n",
    "<center><img src=\"img/2minham.svg\" alt=\"minimization of hamiltonian\"></center>\n",
    "\n",
    "In this notebook we illustratre numerical optimization strategies for minimizing this energy density directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The gradient\n",
    "\n",
    "Any optimization problem relies on an efficient evaluation of the gradient, so the first thing to do is to compute this quantity. The objective function $f$ that we want to minimize is a real function of the complex-valued $A$, or equivalently, of the independent variables $A$ and $\\bar{A}$. The gradient $g$ is then obtained by differentiating $f(\\bar{A},A)$ with respect to $\\bar{A}$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g &= 2 \\times \\frac{\\partial f(\\bar{A},A) }{ \\partial \\bar{A} } \\\\\n",
    "&= 2\\times \\frac{\\partial_{\\bar{A}} \\left \\langle \\Psi(\\bar{A}) \\middle | h  \\middle | \\Psi(A) \\right \\rangle } {\\left \\langle \\Psi(\\bar{A}) \\middle | \\Psi(A)  \\right \\rangle} - 2\\times \\frac{\\left \\langle \\Psi(\\bar{A}) \\middle | h  \\middle | \\Psi(A) \\right \\rangle} {\\left \\langle \\Psi(\\bar{A}) \\middle | \\Psi(A)  \\right \\rangle^2} \\partial_{\\bar{A}} \\left \\langle \\Psi(\\bar{A}) \\middle | \\Psi(A) \\right \\rangle ,\\\\\n",
    "&= 2\\times \\frac{\\partial_{\\bar{A}}  \\left \\langle \\Psi(\\bar{A}) \\middle | h  \\middle | \\Psi(A) \\right \\rangle - e \\partial_{\\bar{A}} \\left \\langle \\Psi(\\bar{A}) \\middle | \\Psi(A)  \\right \\rangle  } {\\left \\langle \\Psi(\\bar{A}) \\middle | \\Psi(A)  \\right \\rangle},\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we have clearly indicated $A$ and $\\bar{A}$ as independent variables and $e$ is the current energy density given by\n",
    "\n",
    "$$\n",
    "e = \\frac{\\left \\langle \\Psi(\\bar{A}) \\middle | h  \\middle | \\Psi(A) \\right \\rangle} {\\left \\langle \\Psi(\\bar{A}) \\middle | \\Psi(A)  \\right \\rangle}.\n",
    "$$\n",
    "\n",
    "If we make sure that the MPS is properly normalized and subtract the current energy density from every term in the hamiltonian, $h \\leftarrow h - e$, the gradient takes on the simple form\n",
    "\n",
    "$$ g = 2 \\times \\partial_{\\bar{A}} \\left \\langle \\Psi(\\bar{A}) \\middle | h  \\middle | \\Psi(A) \\right \\rangle.$$\n",
    "\n",
    "Thus, the gradient is obtained by differentiating the expression\n",
    "\n",
    "<center><img src=\"img/grad.svg\" alt=\"gradient\"></center>\n",
    "\n",
    "with respect to $\\bar{A}$. This gives rise to a sum over all sites, where in every term we differentiate with respect to one tensor $\\bar{A}$ in the bra layer. Differentiating with respect to one $\\bar{A}$ tensor amounts to leaving out that tensor, and interpreting the open legs as outgoing ones, i.e. each term looks like\n",
    "\n",
    "<center><img src=\"img/gradTerm.svg\" alt=\"gradient term\"></center>\n",
    "\n",
    "The full gradient is then obtained as an infinite sum over these terms. By dividing the terms into three different classes and doing some bookkeeping as illustrated below, we can eventually write this sum in a relatively simple closed form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regularize Hamiltonian such that its expectation value is 0.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h::TensorMap{CartesianSpace, 2, 2}`: Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "\"\"\"\n",
    "function reducedHamUniform(h, A, fpts=fixedPoints(A))\n",
    "    # calculate expectation value\n",
    "    e = real(expVal2Uniform(h, A, fpts))\n",
    "    \n",
    "    # substract from hamiltonian\n",
    "    h̃ = h - e * id(domain(h))\n",
    "    \n",
    "    return h̃\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms of the 'center' kind\n",
    "The first kind of terms that arise in the above expression for the gradient are obtained by differentiation with respect to an $\\bar{A}$ tensor on the legs of the Hamiltonian operator. This results in two 'center' terms\n",
    "\n",
    "<center><img src=\"img/centerTerms.svg\" alt=\"center terms\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the value of the center terms.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `term1::TensorMap{CartesianSpace}`: first gradient term as a tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `term2::TensorMap{CartesianSpace}`: second gradient term as a tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "\"\"\"\n",
    "function gradCenterTerms(h̃, A, fpts=fixedPoints(A))\n",
    "    l, r = fpts\n",
    "    \n",
    "    # calculate first contraction\n",
    "    @tensor term1[-1 -2 -3] := l[-1; 1] * r[5; 7] * A[1 3 2] * A[2 4 5] * conj(A[-3 6 7]) * h̃[3 4; -2 6]\n",
    "    \n",
    "    # calculate second contraction\n",
    "    @tensor term2[-1 -2 -3] := l[6; 1] * r[5; -3] * A[1 3 2] * A[2 4 5] * conj(A[6 7 -1]) * h̃[3 4; 7 -2]\n",
    "    \n",
    "    return term1, term2\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms of the 'left' kind\n",
    "For the terms where we leave out an $\\bar{A}$ tensor to the left of $h$, which we will call 'left' terms, we can contract everything to the left of this missing $\\bar{A}$ with the left fixed point $l$, while everything to the right of $h$ can be contracted with right fixed point $r$.\n",
    "\n",
    "In between these two outer parts of the network there remains a region where the regular MPS transfer matrix $E$ is applied a number of times. The action of this region is therefore captured by the operator $E^n$, where the power $n$ is determined by the seperation between the outer left and right parts for the specific term under consideration. When summing all left terms, the outer parts of the contraction always remain the same, while only the power $n$ differs for every term. Thus, summing all left terms corresponds to contracting the operator \n",
    "\n",
    "$$E_\\text{sum} = 1 + E + E^2 + \\dots = \\frac{1}{1-E}$$\n",
    "\n",
    "between the left and right outer parts. Here, we have naively used the geometric series to write the sum in a closed form. However, since by our normalization the transfer matrix has leading eigenvalue $1$, this resulting expression will diverge and is therefore ill-defined. We can get around this by introducing a regularized transfer matrix $\\tilde{E}$ which is defined by subtracting the divergent part,\n",
    "\n",
    "<center><img src=\"img/regTransfer.svg\" alt=\"regularized transfer matrix\"></center>\n",
    "\n",
    "Since we have already shifted the energy density to have a zero expectation value, $h \\leftarrow h - e$, it can easily be verified that the contribution of the leading divergent part vanishes in every left term, meaning that we can simply replace the original transfer matrix by its regularized version without changing any of the terms, and only then take the infinite sum which now has a well defined expression in terms of an inverse,\n",
    "\n",
    "$$ E_\\text{sum} \\rightarrow \\frac{1}{1-\\tilde{E}} \\equiv (1 - E)^P ,$$\n",
    "\n",
    "where we have introduced the pseudo-inverse defined as $(1 - E)^P = (1-\\tilde{E})^{-1}$.\n",
    "\n",
    "Using this notation we can define the partial contraction\n",
    "\n",
    "<center><img src=\"img/Rh.svg\" alt=\"right effective environment\"></center>\n",
    "\n",
    "such that the sum of all left terms equals\n",
    "\n",
    "<center><img src=\"img/leftTerms.svg\" alt=\"left terms\"></center>\n",
    "\n",
    "If we would compute the partial contraction $R_h$ directly by explicitly computing the pseudo-inverse, this would entail a computational complexity $O(D^6)$. Instead, we can define $R_h$ as the solution of a linear system by multiplying both sides of the corresponding definition by $(1-\\tilde{E})$. This results in an equation of the form $Ax = b$, which may be solved for $x$ by using Krylov-based iterative methods such as a Generalized Minimal RESidual (GMRES) algorithm as implemented in [`KrylovKit.linsolve`](https://jutho.github.io/KrylovKit.jl/stable/man/linear/#KrylovKit.linsolve). Note that these methods only require the action of $A = (1-\\tilde{E})$ on a vector and not the full matrix $A$. This action can again be supplied to the linear solver using a function handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement the action of (1 - Ẽ) on a right vector.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `v::TensorMap{CartesianSpace, 1, 1}`: right vector on which (1 - Ẽ) acts, given as a tensor with 2 legs of dimension (D, D), ordered top-bottom.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `vNew::TensorMap{CartesianSpace, 1, 1}`: result of action of (1 - Ẽ) on `v`, given as a tensor with 2 legs of dimension (D, D), ordered top-bottom.\n",
    "\"\"\"\n",
    "function Ẽright(v, A, fpts=fixedPoints(A))\n",
    "    l, r = fpts\n",
    "\n",
    "    # transfermatrix contribution\n",
    "    @tensor transfer[-1; -2] := A[-1 2 1] * conj(A[-2 2 3]) * v[1; 3]\n",
    "\n",
    "    # fixed point contribution\n",
    "    fixed = tr(l * v) * r\n",
    "\n",
    "    # sum these with the contribution of the identity\n",
    "    vNew = v - transfer + fixed\n",
    "\n",
    "    return vNew\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the partial contraction for Rh.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `Rh::TensorMap{CartesianSpace, 1, 1}`: right partial contraction as a tensor with 2 legs of dimension (D, D), ordered top-bottom.\n",
    "\"\"\"\n",
    "function RhUniform(h̃, A, fpts=fixedPoints(A))\n",
    "    l, r = fpts\n",
    "    \n",
    "    # construct b, which is the matrix to the right of (1 - E)^P in the figure above\n",
    "    @tensor b[-1; -2] := r[4; 5] * A[-1 2 1] * A[1 3 4] * conj(A[-2 8 7]) * conj(A[7 6 5]) * h̃[2 3; 8 6]\n",
    "    \n",
    "    # solve Ax = b for x\n",
    "    Rh, _ = linsolve(v -> Ẽright(v, A, fpts), b)\n",
    "    \n",
    "    return Rh\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the value of the left terms.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `leftTerms::TensorMap{CartesianSpace}`: left terms of gradient as a tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "\"\"\"\n",
    "function gradLeftTerms(h̃, A, fpts=fixedPoints(A))\n",
    "    l, r = fpts\n",
    "    \n",
    "    # calculate partial contraction\n",
    "    Rh = RhUniform(h̃, A, fpts)\n",
    "    \n",
    "    # calculate full contraction\n",
    "    @tensor leftTerms[-1 -2 -3] := Rh[1; -3] * A[2 -2 1] * l[-1; 2]\n",
    "    \n",
    "    return leftTerms\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms of the 'right' kind\n",
    "\n",
    "In a similar way, the terms where we leave out an $\\bar{A}$ to the right of  $h$ can be evaluated by defining the partial contraction\n",
    "\n",
    "<center><img src=\"img/Lh.svg\" alt=\"Lh\"></center>\n",
    "\n",
    "which can again be found by solving a linear system, such that the sum of all right terms can be written as\n",
    "\n",
    "<center><img src=\"img/rightTerms.svg\" alt=\"rightTerms\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement the action of (1 - Ẽ) on a left vector.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `v::TensorMap{CartesianSpace, 1, 1}`: left vector on which (1 - Ẽ) acts, given as a tensor with 2 legs of dimension (D, D), ordered bottom-top.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `vNew::TensorMap{CartesianSpace, 1, 1}`: result of action of (1 - Ẽ) on `v`, given as a tensor with 2 legs of dimension (D, D), ordered bottom-top.\n",
    "\"\"\"\n",
    "function Ẽleft(v, A, fpts=fixedPoints(A))\n",
    "    l, r = fpts\n",
    "\n",
    "    # transfer matrix contribution\n",
    "    @tensor transfer[-1; -2] := v[3; 1] * A[1 2 -2] * conj(A[3 2 -1])\n",
    "\n",
    "    # fixed point contribution\n",
    "    fixed = tr(v * r) * l\n",
    "\n",
    "    # sum these with the contribution of the identity\n",
    "    vNew = v - transfer + fixed\n",
    "\n",
    "    return vNew\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the partial contraction for Lh.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `Lh::TensorMap{CartesianSpace, 1, 1}`: left partial contraction as a tensor with 2 legs of dimension (D, D), ordered bottom-top.\n",
    "\"\"\"\n",
    "function LhUniform(h̃, A, fpts=fixedPoints(A))\n",
    "    l, r = fpts\n",
    "    \n",
    "    # construct b, which is the matrix to the right of (1 - E)^P in the figure above\n",
    "    @tensor b[-1; -2] := l[5; 1] * A[1 3 2] * A[2 4 -2] * conj(A[5 6 7]) * conj(A[7 8 -1]) * h̃[3 4; 6 8]    \n",
    "    \n",
    "    # solve Ax = b for x\n",
    "    Lh, _ = linsolve(v -> Ẽleft(v, A, fpts), b)\n",
    "    \n",
    "    return Lh\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the value of the right terms.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `rightTerms::TensorMap{CartesianSpace}`: right terms of gradient as a tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "\"\"\"\n",
    "function gradRightTerms(h̃, A, fpts=fixedPoints(A))\n",
    "    l, r = fpts\n",
    "    \n",
    "    # calculate partial contraction\n",
    "    Lh = LhUniform(h̃, A, fpts)\n",
    "    \n",
    "    # calculate full contraction\n",
    "    @tensor rightTerms[-1 -2 -3] := Lh[-1; 1] * A[1 -2 2] * r[2; -3]\n",
    "    \n",
    "    return rightTerms\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gradient\n",
    "\n",
    "The full gradient is then found by summing the contributions of all three types of terms,\n",
    "\n",
    "<center><img src=\"img/gradFull.svg\" alt=\"gradient\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the gradient of the expectation value of a given Hamiltonian.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `fpts::Tuple=fixedPoints(A)`: left and right fixed points of transfermatrix, normalized.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `grad::TensorMap{CartesianSpace}`: gradient as a tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "\"\"\"\n",
    "function gradient(h, A, fpts=fixedPoints(A))\n",
    "        \n",
    "    # renormalize Hamiltonian\n",
    "    h̃ = reducedHamUniform(h, A, fpts)\n",
    "        \n",
    "    # find terms\n",
    "    centerTerm1, centerTerm2 = gradCenterTerms(h̃, A, fpts)\n",
    "    leftTerms = gradLeftTerms(h̃, A, fpts)\n",
    "    rightTerms = gradRightTerms(h̃, A, fpts)\n",
    "    \n",
    "    grad = 2 * (centerTerm1 + centerTerm2 + leftTerms + rightTerms)\n",
    "    \n",
    "    return grad\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient descent algorithms\n",
    "\n",
    "The most straightforward way to use this expression for the gradient to find the ground state of a Hamiltonian is to implement a gradient-search method for minimizing the energy expecation value. The simplest such method is a steepest-descent search, where in every iteration the tensor $A$ is updated in the direction opposite to the gradient along a small step $\\varepsilon$,\n",
    "\n",
    "$$ A_{i+1} = A_i - \\varepsilon g .$$\n",
    "\n",
    "This procedure is repeated until we find the optimal MPS tensor $A^*$ for which the gradient vanishes. This approach can be improved upon by resorting to other optimization schemes such a conjugate-gradient or quasi-Newton methods. Below we demonstrate both a simple steepest-descent with a fixed step size, as well as an approach using optimization routines supplied by the Julia package [OptimKit.jl](https://github.com/Jutho/OptimKit.jl) through the `OptimKit.optimize` routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the ground state of a given Hamiltonian using gradient descent.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h::TensorMap{CartesianSpace, 2, 2}`: Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `D::Int`: bond dimension.\n",
    "- `A0::TensorMap{CartesianSpace}`: normalized MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `eps::Float64`: stepsize.\n",
    "- `tol::Float64`: tolerance for convergence criterium.\n",
    "- `maxIter::Int`: maximum number of iterations.\n",
    "- `verbose::Bool`: print progress.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `E::Float64`: expectation value @ minimum\n",
    "- `A::TensorMap{CartesianSpace}`: ground state MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "\"\"\"\n",
    "function groundStateGradDescent(\n",
    "    h,\n",
    "    D,\n",
    "    A0=createMPS(D, dim(space(h, 1)));\n",
    "    eps=1e-1,\n",
    "    tol=1e-4,\n",
    "    maxIter=1e4,\n",
    "    verbose=true\n",
    ")\n",
    "    \n",
    "    # initialize\n",
    "    g = gradient(h, A0)\n",
    "    A = A0\n",
    "    i = 0\n",
    "\n",
    "    while norm(g) > tol\n",
    "        # do a step\n",
    "        A = A - eps * g\n",
    "        A = normalizeMPS(A)\n",
    "        i += 1\n",
    "        \n",
    "        if verbose && (i % 100 == 0)\n",
    "            E = real(expVal2Uniform(h, A))\n",
    "            @printf \"iteration:\\t%d,\\tenergy:\\t%.12f\\tgradient norm\\t%.4e\\n\"  i E norm(g)\n",
    "        end\n",
    "\n",
    "        # calculate new gradient\n",
    "        g = gradient(h, A)\n",
    "        \n",
    "        if i > maxIter\n",
    "            println(\"Warning: gradient descent did not converge after $maxIter iterations!\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # calculate ground state energy\n",
    "    E = real(expVal2Uniform(h, A))\n",
    "    \n",
    "    return E, A\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `OptimKit.optimize` in this context, where we are dealing with an objective function that takes a complex-valued tensor as input and returns a real scalar, we must be careful to correctly treat the real and imaginary parts of the tensor entries as independent variational parameters. Specifically, we need to supply `OptimKit.optimize` with an appropriate inner product `inner(x, ξ1, ξ2)` which computes the inner product between two gradients or similar objects at position `x`. The `x` dependence is useful for optimization on manifolds, but can be ignored for our current purpose. Keeping in mind that we want to treat the real and imaginary parts of the `ξ` independently, we will define `inner` to compute the sum of the conventional inner products of the real and imaginary parts respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the ground state using `OptimKit.optimize`.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h::TensorMap{CartesianSpace, 2, 2}`: Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `D::Int`: bond dimension.\n",
    "- `A0::TensorMap{CartesianSpace}`: normalized MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `tol::Float64`: tolerance for convergence criterium.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `E::Float64`: expectation value @ minimum\n",
    "- `A::TensorMap{CartesianSpace}`: ground state MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "\"\"\"\n",
    "function groundStateMinimize(h, D, A0=createMPS(D, dim(space(h, 1))); tol=1e-4)\n",
    "    \n",
    "    # define function to optimize with OptimKit.optimize\n",
    "    \"\"\"\n",
    "    Function to optimize via `OptimKit.optimize`.\n",
    "    \n",
    "    ### Arguments\n",
    "\n",
    "    - `A::TensorMap{CartesianSpace}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "\n",
    "    ### Returns\n",
    "\n",
    "    - `e::Float64`: function value @ `A`\n",
    "    - `g::TensorMap{CartesianSpace}`: gradient as a tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "    \"\"\"\n",
    "    function f(A)\n",
    "        A = normalizeMPS(A)\n",
    "        \n",
    "        # calculate fixed points\n",
    "        fpts = fixedPoints(A)\n",
    "        \n",
    "        # calculate function value and gradient\n",
    "        e = real(expVal2Uniform(h, A, fpts))\n",
    "        g = gradient(h, A, fpts)\n",
    "        \n",
    "        return e, g\n",
    "    end\n",
    "    \n",
    "    # specify inner product that treats real and imaginary parts of variational parameters\n",
    "    # as completely independent variables\n",
    "    myinner(x, ξ1, ξ2) = dot(real(ξ1), real(ξ2)) + dot(imag(ξ1), imag(ξ2))\n",
    "\n",
    "    # calculate minimum using conjugte gradient optimization\n",
    "    A, E, _ = optimize(f, A0, ConjugateGradient(; gradtol=tol, verbosity=1), inner=myinner)\n",
    "    \n",
    "    return E, A\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate these methods, we now have a look the specific case of the antiferromagnetic spin-1 Heisenberg model in one dimension. To this end we first define the spin-1 Heisenberg Hamiltonian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Construct the spin-1 Heisenberg Hamiltonian for given couplings.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `Jx::Float64`: coupling strength in x direction.\n",
    "- `Jy::Float64`: coupling strength in y direction.\n",
    "- `Jz::Float64`: coupling strength in z direction.\n",
    "- `hz::Float64`: coupling for Sz terms.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `h::TensorMap{CartesianSpace, 2, 2}`: Spin-1 Heisenberg Hamiltonian.\n",
    "\"\"\"\n",
    "function Heisenberg(Jx, Jy, Jz, hz)\n",
    "    \n",
    "    Sx = TensorMap(ComplexF64[0 1 0; 1 0 1; 0 1 0] ./ sqrt(2), ℝ^3 ← ℝ^3)\n",
    "    Sy = TensorMap(ComplexF64[0 -im 0; im 0 -im; 0 im 0] ./ sqrt(2), ℝ^3 ← ℝ^3)\n",
    "    Sz = TensorMap(ComplexF64[1 0 0; 0 0 0; 0 0 -1], ℝ^3 ← ℝ^3)\n",
    "    I = id(Matrix{ComplexF64}, ℝ^3)\n",
    "\n",
    "    return -Jx * Sx ⊗ Sx - Jy * Sy ⊗ Sy - Jz * Sz ⊗ Sz - hz/2 * (Sz ⊗ I + I ⊗ Sz)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, D = 3, 12\n",
    "A = createMPS(D, d)\n",
    "\n",
    "h = Heisenberg(-1, -1, -1, 0)\n",
    "\n",
    "# energy optimization using naive gradient descent\n",
    "# for D=12 or higher: tolerance lower than 5e-3 gives very long runtimes\n",
    "println(\"Gradient descent optimization:\\n\")\n",
    "t = @elapsed E1, A1 = groundStateGradDescent(h, D, A; eps=1e-1, tol=5e-3, maxIter=1e4)\n",
    "println(\"Time until convergence: $(t)s\")\n",
    "println(\"Computed energy: $(E1)\\n\")\n",
    "\n",
    "# energy optimization using OptimKit.optimize\n",
    "println(\"Optimization using OptimKit.optimize:\\n\")\n",
    "t0 = time()\n",
    "t = @elapsed E2, A2 = groundStateMinimize(h, D, A; tol=1e-5)\n",
    "println(\"Time until convergence: $(t)s\")\n",
    "print(\"Computed energy: $(E2)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The VUMPS algorithm\n",
    "\n",
    "In the previous section we have derived an expression for the gradient starting from an MPS in the uniform gauge, which corresponds to an object that lives in the space of MPS tensors. We now discuss how to improve upon direct optimization schemes based on this form of the gradient by exploiting the structure of the MPS manifold as well as the mixed gauge for MPS.\n",
    "\n",
    "Indeed, while the gradient in the above form indicates a direction in the space of complex tensors in which the energy decreases, intuitively it would make more sense if we could find a way to interpret the gradient as a direction *along the MPS manifold* along which we can decrease the energy. This can be achieved by interpreting the gradient as a *tangent vector in the tangent space to the MPS manifold*. By formulating the energy optimization in terms of this tangent space gradient written in mixed gauge, one arives at the [VUMPS](https://doi.org/10.1103/PhysRevB.97.045145) algorithm (which stand for 'variational uniform matrix product states'). The precise derivation of the tangent space gradient in mixed gauge falls beyond the scope of this tutorial, and can be found in the [lecture notes](https://doi.org/10.21468/SciPostPhysLectNotes.7). Instead we will simply illustrate the implementation of the VUMPS algorithm given the mixed gauge tangent space gradient.\n",
    "\n",
    "Most of the following required steps will be reminiscent of those outlined above, where we now consistently work in the mixed gauge. We start off by implementing the regularization of a two-site Hamiltonian in the mixed gauge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regularize Hamiltonian such that its expectation value is 0.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h::TensorMap{CartesianSpace, 2, 2}`: Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `Ac::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, center gauge.\n",
    "- `Ar::TensorMap{CartesianSpace, 1, 2}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, right orthonormal.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "\"\"\"\n",
    "function reducedHamMixed(h, Ac, Ar)\n",
    "    # calculate expectation value\n",
    "    e = real(expVal2Mixed(h, Ac, Ar))\n",
    "    \n",
    "    # substract from hamiltonian\n",
    "    h̃ = h - e * id(domain(h))\n",
    "    \n",
    "    return h̃\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational optimum of the energy is characterized by the condition that the gradient is zero at this point. Writing the tangent space gradient as $G$, we now wish to formulate an algorithm which minimizes the error measure\n",
    "\n",
    "$$ \\varepsilon = \\left( \\boldsymbol{G}^\\dagger \\boldsymbol{G} \\right)^{1/2} $$\n",
    "\n",
    "in an efficient way. The explicit form of the tangent space gradient in mixed gauge is given by\n",
    "\n",
    "$$ G = A^\\prime_{C} - A_L C^\\prime = A^\\prime_{C} - C^\\prime A_R, $$\n",
    "\n",
    "where $A^\\prime_{C}$ and $C^\\prime$ are defined as\n",
    "\n",
    "<center><img src=\"img/Acprime.svg\" alt=\"Ac prime\"></center>\n",
    "\n",
    "and\n",
    "\n",
    "<center><img src=\"img/Cprime.svg\" alt=\"C prime\"></center>\n",
    "\n",
    "Here, we again use $L_h$ and $R_h$ to indicate the partial contractions\n",
    "\n",
    "<center><img src=\"img/LhMixed.svg\" alt=\"Lh mixed gauge\"></center>\n",
    "\n",
    "and\n",
    "\n",
    "<center><img src=\"img/RhMixed.svg\" alt=\"Rh mixed gauge\"></center>\n",
    "\n",
    "where the transfer matrices $E^L_L$ and $E^R_R$ appearing in these expressions now contain only left-gauged and right-gauged MPS tensors $A_L$ and $A_R$ respectively.\n",
    "\n",
    "If we interpret the two terms appearing in the tangent space gradient as defining the effective Hamiltonians $H_{A_C}(\\cdot)$ and $H_C(\\cdot)$ such that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_{A_C}(A_C) = A_C^\\prime \\\\\n",
    "H_C(C) = C^\\prime ,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we can characterize the variational optimum in terms of the fixed points of these operators. Indeed, since the tangent space gradient should be zero at the variational optimum, this point satisfies $A_C' = A_L C' = C' A_R$. This implies that the optimal MPS should obey the following set of equations,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_{A_C}(A_C) \\propto A_C \\\\\n",
    "H_C(C) \\propto C \\\\\n",
    "A_C = A_L C = C A_R ,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "meaning that the optimal MPS should correspond to a fixed point of the effective Hamiltonians $H_{A_C}$ and $H_C$ and satisfy the mixed gauge condition. The VUMPS algorithm then consists of an iterative method for finding a set $\\{A_L, A_C, A_R, C\\}$ that satisfies these equations simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the required operators\n",
    "\n",
    "Similar to before, we again have to compute the contributions of the left and right environment terms $L_h$ and $R_h$ given above. We therefore require function handles defining the action of the left (resp. right) transfer matrix $E^L_L$ (resp. $E^R_R$) on a left (resp. right) matrix. To this end, we can simply reuse the implementations `ẼLeft` and `ẼRight` defined above, if we take into account that the left (resp. right) fixed point of $E^L_L$ (resp. $E^R_R$) is the identity while its right (resp. left) fixed point is precisely $C C^\\dagger$ (resp. $C^\\dagger C$). This last fact follows immediately from the mixed gauge condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate Lh, for a given MPS in mixed gauge.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `Al::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, left orthonormal.\n",
    "- `C::TensorMap{CartesianSpace, 1, 1}`: center gauge tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "- `tol::Float64=1e-5`: tolerance for linear solver.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `Lh::TensorMap{CartesianSpace, 1, 1}`: left partial contraction as a tensor with 2 legs of dimension (D, D), ordered bottom-top.\n",
    "\"\"\"\n",
    "function LhMixed(h̃, Al, C; tol=1e-5)\n",
    "    tol = max(tol, 1e-14)\n",
    "    \n",
    "    # construct fixed points for Al\n",
    "    l = id(space(Al, 1)) # left fixed point of left transfer matrix: left orthonormal\n",
    "    r = C * C' # right fixed point of left transfer matrix\n",
    "        \n",
    "    # construct b\n",
    "    @tensor b[-1; -2] := Al[4 2; 1] * Al[1 3; -2] * conj(Al[4 5; 6]) * conj(Al[6 7; -1]) * h̃[2 3; 5 7]\n",
    "    \n",
    "    # solve a x = b for x\n",
    "    Lh, _ = linsolve(v -> Ẽleft(v, Al, (l, r)), b; tol)\n",
    "    \n",
    "    return Lh\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate Rh, for a given MPS in mixed gauge.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `Ar::TensorMap{CartesianSpace, 1, 2}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, right orthonormal.\n",
    "- `C::TensorMap{CartesianSpace, 1, 1}`: center gauge tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "- `tol::Float64=1e-5`: tolerance for linear solver.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `Rh::TensorMap{CartesianSpace, 1, 1}`: right partial contraction as a tensor with 2 legs of dimension (D, D), ordered top-bottom.\n",
    "\"\"\"\n",
    "function RhMixed(h̃, Ar, C; tol=1e-5)\n",
    "    tol = max(tol, 1e-14)\n",
    "    \n",
    "    # construct fixed points for Ar\n",
    "    l = C' * C # left fixed point of right transfer matrix\n",
    "    r = id(space(Ar, 3)) # right fixed point of right transfer matrix: right orthonormal\n",
    "\n",
    "    # construct b\n",
    "    @tensor b[-1; -2] := Ar[-1; 2 1] * Ar[1; 3 4] * conj(Ar[-2; 7 6]) * conj(Ar[6; 5 4]) * h̃[2 3; 7 5]\n",
    "    \n",
    "    # solve ax = b for x\n",
    "    Rh, _ = linsolve(v -> Ẽright(v, Ar, (l, r)), b; tol)\n",
    "    \n",
    "    return Rh\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement the actions of the effective Hamiltonians $H_{A_C}$ and $H_{C}$ defined above,\n",
    "\n",
    "<center><img src=\"img/H_Ac.svg\" alt=\"H_Ac\"></center>\n",
    "\n",
    "<center><img src=\"img/H_C.svg\" alt=\"H_C\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Action of the effective Hamiltonian for Ac (131) on a vector.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `v::TensorMap{CartesianSpace, 2, 1}`: vector on which the effective Hamiltonian acts, given as a tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `Al::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, left orthonormal.\n",
    "- `Ar::TensorMap{CartesianSpace, 1, 2}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, right orthonormal.\n",
    "- `Lh::TensorMap{CartesianSpace, 1, 1}`: left environment tensor with 2 legs of dimension (D, D), ordered bottom-top.\n",
    "- `Rh::TensorMap{CartesianSpace, 1, 1}`: right environment tensor with 2 legs of dimension (D, D), ordered top-bottom.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `H_AcV::TensorMap{CartesianSpace, 2, 1}`: result of the action of `H_Ac` on `v`, given as a tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "\"\"\"\n",
    "function H_Ac(v, h̃, Al, Ar, Lh, Rh)\n",
    "    # first term\n",
    "    @tensor term1[-1 -2; -3] := Al[4 2; 1] * v[1 3; -3] * conj(Al[4 5; -1]) * h̃[2 3; 5 -2]\n",
    "\n",
    "    # second term\n",
    "    @tensor term2[-1 -2; -3] := v[-1 2; 1] * Ar[1; 3 4] * conj(Ar[-3; 5 4]) * h̃[2 3; -2 5]\n",
    "\n",
    "    # third term\n",
    "    @tensor term3[-1 -2; -3] := Lh[-1; 1] * v[1 -2; -3]\n",
    "\n",
    "    # fourth term\n",
    "    @tensor term4[-1 -2; -3] := v[-1 -2; 1] * Rh[1; -3]\n",
    "\n",
    "    # sum\n",
    "    H_AcV = term1 + term2 + term3 + term4\n",
    "\n",
    "    return H_AcV\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Action of the effective Hamiltonian for C (132) on a vector.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `v::TensorMap{CartesianSpace, 1, 1}`: vector on which the effective Hamiltonian acts, given as a tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `Al::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, left orthonormal.\n",
    "- `Ar::TensorMap{CartesianSpace, 1, 2}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, right orthonormal.\n",
    "- `Lh::TensorMap{CartesianSpace, 1, 1}`: left environment tensor with 2 legs of dimension (D, D), ordered bottom-top.\n",
    "- `Rh::TensorMap{CartesianSpace, 1, 1}`: right environment tensor with 2 legs of dimension (D, D), ordered top-bottom.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `H_CV::TensorMap{CartesianSpace, 1, 1}`: result of the action of `H_C` on `v`, given as a tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "\"\"\"\n",
    "function H_C(v, h̃, Al, Ar, Lh, Rh)\n",
    "    # first term\n",
    "    @tensor term1[-1; -2] := Al[5 3; 1] * v[1; 2] * Ar[2; 4 7] * conj(Al[5 6; -1]) * conj(Ar[-2; 8 7]) * h̃[3 4; 6 8]\n",
    "\n",
    "    # second term\n",
    "    term2 = Lh * v\n",
    "\n",
    "    # third term\n",
    "    term3 = v * Rh\n",
    "\n",
    "    # sum\n",
    "    @tensor H_CV = term1 + term2 + term3\n",
    "\n",
    "    return H_CV\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the VUMPS algorithm\n",
    "\n",
    "In order to find a set $\\{A_L^*, A_C^*, A_R^*, C^*\\}$ that satisfies the VUMPS fixed point equations given above, we use an iterative method in which each iteration consists of the following steps, each time starting from a given set $\\{A_L, A_C, A_R, C\\}$:\n",
    "\n",
    "1. Solve the eigenvalue equations for $H_{A_C}$ and $H_C$, giving new center tensors $\\tilde{A}_C$ and $\\tilde{C}$.\n",
    "\n",
    "2. From these new center tensors, construct a set $\\{\\tilde{A}_L, \\tilde{A}_R, \\tilde{A}_C, \\tilde{C}\\}$.\n",
    "\n",
    "3. Update the set of tensors $\\{A_L, A_C, A_R, C\\} \\leftarrow \\{\\tilde{A}_L, \\tilde{A}_C, \\tilde{A}_R, \\tilde{C}\\}$ and evaluate the norm of the gradient $\\varepsilon = \\left | \\left | H_{A_C} (A_C) - A_L H_C(C) \\right | \\right |$.\n",
    "\n",
    "4. If the norm of the gradient lies above the given tolerance, repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Updating the center tensors\n",
    "\n",
    "We start by defining a routine `calcNewCenter` which finds the new center tensors $\\tilde{A}_C$ and $\\tilde{C}$ by solving the eigenvalue problem defined by the effective Hamiltonians implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find a new guess for Ac and C as fixed points of the maps H_Ac and H_C.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `Al::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, left orthonormal.\n",
    "- `Ac::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, center gauge.\n",
    "- `Ar::TensorMap{CartesianSpace, 1, 2}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, right orthonormal.\n",
    "- `C::TensorMap{CartesianSpace, 1, 1}`: center gauge tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "- `Lh::TensorMap{CartesianSpace, 1, 1}`: left environment tensor with 2 legs of dimension (D, D), ordered bottom-top.\n",
    "- `Rh::TensorMap{CartesianSpace, 1, 1}`: right environment tensor with 2 legs of dimension (D, D), ordered top-bottom.\n",
    "- `tol::Float64=1e-5`: current tolerance.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `Ãc::TensorMap{CartesianSpace, 2, 1}`: new center gauge MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `C̃::TensorMap{CartesianSpace, 1, 1}`: new center gauge tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "\"\"\"\n",
    "function calcNewCenter(h̃, Al, Ac, Ar, C, Lh=nothing, Rh=nothing; tol=1e-5)\n",
    "    tol = max(tol, 1e-14)\n",
    "    \n",
    "    # calculate left en right environment if they are not given\n",
    "    isnothing(Lh) && (Lh = LhMixed(h̃, Al, C; tol))\n",
    "    isnothing(Rh) && (Rh = RhMixed(h̃, Ar, C; tol))\n",
    "    \n",
    "    # calculate new Ãc\n",
    "    _, vecs, _ = eigsolve(v -> H_Ac(v, h̃, Al, Ar, Lh, Rh), Ac, 1, :SR; tol)\n",
    "    Ãc = vecs[1]\n",
    "\n",
    "    # calculate new C̃\n",
    "    _, vecs, _ = eigsolve(v -> H_C(v, h̃, Al, Ar, Lh, Rh), C, 1, :SR; tol)\n",
    "    C̃ = vecs[1]\n",
    "    \n",
    "    return Ãc, C̃\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract a new set of mixed-gauge MPS tensors\n",
    "\n",
    "Once we have new center tensors, we can use these to construct a new set of mixed-gauge MPS tensors. To do this in a stable way, we will determine the global updates $\\tilde{A}_L$ and $\\tilde{A}_R$ as the left and right isometric tensors that minimize\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\varepsilon_L = \\min ||\\tilde{A}_C - \\tilde{A}_L \\tilde{C}||_2 \\\\\n",
    "\\varepsilon_R = \\min ||\\tilde{A}_C - \\tilde{C} \\tilde{A}_L||_2 .\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This can be achieved in a robust and close to optimal way by making use of the left and right polar decompositions\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{A}_C = U^l_{A_C} P^l_{A_C}, \\qquad \\tilde{C} = U^l_{C} P^l_{C}, \\\\\n",
    "\\tilde{A}_C = P^r_{A_C}  U^r_{A_C} , \\qquad \\tilde{C} = P^r_{C} U^r_{C},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "to obtain\n",
    "\n",
    "$$ \\tilde{A}_L = U^l_{A_C} (U^l_C)^\\dagger, \\qquad \\tilde{A}_R = (U^r_C)^\\dagger U^r_{A_C}. $$\n",
    "\n",
    "In order to give the  procedure some additional stability, we may also choose to use the $\\tilde{A}_L$ obtained with these polar decompositions to compute the tensors $\\tilde{A}_R$ and $\\tilde{A}_C$ by right orthonormalization of this $\\tilde{A}_L$. This approach ensures that the MPS satisfies the mixed gauge condition at all times, improving the overal stabilitiy of the VUMPS algorithm. This procedure is implemented in the `minAcC` routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find Al and Ar corresponding to Ãc and C̃, according to algorithm 5 in the lecture notes.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `Ãc::TensorMap{CartesianSpace, 2, 1}`: new guess for center gauge MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `C̃::TensorMap{CartesianSpace, 1, 1}`: new guess for center gauge tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "- `tol::Float64=1e-5`: canonicalization tolerance.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `Al::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, left orthonormal.\n",
    "- `Ar::TensorMap{CartesianSpace, 1, 2}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, right orthonormal.\n",
    "- `C::TensorMap{CartesianSpace, 1, 1}`: center gauge tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "\"\"\"\n",
    "function minAcC(Ãc, C̃; tol=1e-5)\n",
    "    tol = max(tol, 1e-14)\n",
    "\n",
    "    # polar decomposition of Ac\n",
    "    UlAc, _ = leftorth(Ãc, (1, 2), (3,); alg=Polar())\n",
    "                    \n",
    "    # polar decomposition of C\n",
    "    UlC, _ = leftorth(C̃, (1,), (2,); alg=Polar())\n",
    "    \n",
    "    # construct Al\n",
    "    Al = UlAc * UlC'\n",
    "    \n",
    "    # find corresponding Ar, C, and Ac through right orthonormalizing Al\n",
    "    C, Ar = rightOrthonormalize(Al, C̃; tol)\n",
    "    nrm = tr(C * C')\n",
    "    C /= sqrt(nrm)\n",
    "    @tensor Ac[-1 -2; -3] := Al[-1 -2; 1] * C[1; -3]\n",
    "    \n",
    "    return Al, Ac, Ar, C\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the norm of the gradient\n",
    "\n",
    "As a last step, we use the routine `gradientNorm` to compute the norm of the tangent space gradient in order to check if the procedure has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the norm of the gradient.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h̃::TensorMap{CartesianSpace, 2, 2}`: reduced Hamiltonian as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `Al::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, left orthonormal.\n",
    "- `Ac::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, center gauge.\n",
    "- `Ar::TensorMap{CartesianSpace, 1, 2}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, right orthonormal.\n",
    "- `C::TensorMap{CartesianSpace, 1, 1}`: center gauge tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "- `Lh::TensorMap{CartesianSpace, 1, 1}`: left environment tensor with 2 legs of dimension (D, D), ordered bottom-top.\n",
    "- `Rh::TensorMap{CartesianSpace, 1, 1}`: right environment tensor with 2 legs of dimension (D, D), ordered top-bottom.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `norm::Float64`: norm of the gradient @Al, Ac, Ar, C\n",
    "\"\"\"\n",
    "function gradientNorm(h̃, Al, Ac, Ar, C, Lh, Rh)\n",
    "    # calculate update on Ac and C using maps H_Ac and H_c\n",
    "    AcUpdate = H_Ac(Ac, h̃, Al, Ar, Lh, Rh)\n",
    "    CUpdate = H_C(C, h̃, Al, Ar, Lh, Rh)\n",
    "    @tensor AlCupdate[-1 -2; -3] := Al[-1 -2; 1] * CUpdate[1; -3]\n",
    "    \n",
    "    return norm(AcUpdate - AlCupdate)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this allows to implement the VUMPS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the ground state of a given Hamiltonian using VUMPS.\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `h::TensorMap{CartesianSpace, 2, 2}`: Hamiltonian to optimize as a tensor with 4 legs of dimension (d, d, d, d), ordered topLeft-topRight-bottomLeft-bottomRight.\n",
    "- `D::Int`: bond dimension.\n",
    "- `A0::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right.\n",
    "- `tol::Float64=1e-4`: relative convergence criterium.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `E::Float64`: expectation value @ minimum\n",
    "- `Al::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, left orthonormal.\n",
    "- `Ac::TensorMap{CartesianSpace, 2, 1}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, center gauge.\n",
    "- `Ar::TensorMap{CartesianSpace, 1, 2}`: MPS tensor with 3 legs of dimension (D, d, D), ordered left-bottom-right, right orthonormal.\n",
    "- `C::TensorMap{CartesianSpace, 1, 1}`: center gauge tensor with 2 legs of dimension (D, D), ordered left-right.\n",
    "\"\"\"\n",
    "function vumps(h, D, A0=createMPS(D, dim(space(h, 1))); tol=1e-4, tolFactor=1e-1, verbose=true)    \n",
    "    # go to mixed gauge\n",
    "    Al, Ac, Ar, C = mixedCanonical(A0)\n",
    "    \n",
    "    flag = true\n",
    "    delta = 1e-5\n",
    "    i = 0\n",
    "    \n",
    "    while flag\n",
    "        i += 1\n",
    "        \n",
    "        # regularize H\n",
    "        h̃ = reducedHamMixed(h, Ac, Ar)\n",
    "        \n",
    "        # calculate environments\n",
    "        Lh = LhMixed(h̃, Al, C; tol=delta*tolFactor)\n",
    "        Rh = RhMixed(h̃, Ar, C; tol=delta*tolFactor)\n",
    "        \n",
    "        # calculate norm\n",
    "        delta = gradientNorm(h̃, Al, Ac, Ar, C, Lh, Rh)\n",
    "        \n",
    "        # check convergence\n",
    "        delta < tol && (flag = false)\n",
    "        \n",
    "        # calculate new center\n",
    "        Ãc, C̃ = calcNewCenter(h̃, Al, Ac, Ar, C, Lh, Rh; tol=delta*tolFactor)\n",
    "        \n",
    "        # find Al, Ar from Ãc, C̃\n",
    "        Ãl, Ãc, Ãr, C̃ = minAcC(Ãc, C̃; tol=delta*tolFactor^2)\n",
    "        \n",
    "        # update tensors\n",
    "        Al, Ac, Ar, C = Ãl, Ãc, Ãr, C̃\n",
    "        \n",
    "        # print current energy\n",
    "        if verbose\n",
    "            E = real(expVal2Mixed(h, Ac, Ar))\n",
    "            @printf \"iteration:\\t%d\\tenergy:\\t%.12f\\tgradient norm:\\t%.4e\\n\" i E delta\n",
    "        end\n",
    "    end\n",
    "    E = real(expVal2Mixed(h, Ac, Ar))\n",
    "\n",
    "    return E, Al, Ac, Ar, C\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again test this implementation on the spin-1 Heisenberg antiferromagnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, D = 3, 12\n",
    "A = createMPS(D, d)\n",
    "A = normalizeMPS(A)\n",
    "\n",
    "h = Heisenberg(-1, -1, -1, 0)\n",
    "\n",
    "# energy optimization using VUMPS\n",
    "println(\"Energy optimization using VUMPS:\\n\")\n",
    "t0 = time()\n",
    "t = @elapsed E, Al, Ac, Ar, C = vumps(h, D, A; tol=1e-4, tolFactor=1e-2, verbose=true)\n",
    "println(\"\\nTime until convergence: $(t)s\")\n",
    "print(\"Computed energy: $(E)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained this ground state MPS, it is worthwile to have a look at the corresponding entanglement spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, S, _ = tsvd(C, (1,), (2,)) # singular values of center matrix give entanglement spectrum\n",
    "S = diag(S[])\n",
    "scatter(1:length(S), S, title=\"Entanglement spectrum of ground state\", marker=:x, yaxis=:log, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the entanglement spectrum consists of degenerate groups, which reflects an underlying symmetry in the ground state of the spin-1 Heisenberg antiferromagnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Elementary excitations\n",
    "\n",
    "#### Quasiparticle ansatz\n",
    "\n",
    "The methods described above can be extended beyond computing the ground state. We briefly discuss how one can also study excitations on top of a given ground state. For this, we introduce the MPS quasiparticle ansatz, given by\n",
    "\n",
    "<center><img src=\"img/excitation.svg\" alt=\"quasiparticle ansatz\"></center>\n",
    "\n",
    "This ansatz cosists of defining a new state by changing one $A$ tensor of the ground state at site $n$ and taking a momentum superposition.\n",
    "\n",
    "Before describing how to optimize the tensor $B$, it is worthwile to investigate the corresponding variational space in a bit more detail. First, we note that this excitation ansatz can be interpreted as nothing more than a boosted version of a tangent vector to the MPS manifold. In particular, this means that we will be able to apply all kinds of useful tricks and manipulations to the tensor $B$ (cfr. the [lecture notes](https://doi.org/10.21468/SciPostPhysLectNotes.7) for an introduction to tangent vectors and their properties). For example, we can see that $B$ has gauge degrees of freedom, as the corresponding excited state is invariant under an additive gauge transformation of the form\n",
    "\n",
    "<center><img src=\"img/gaugeExcitation.svg\" alt=\"gauge transform excitation\"></center>\n",
    "\n",
    "where $Y$ is an arbitrary $D \\times D$ matrix. This gauge freedom can be eliminated, thereby removing the zero modes in the variational subspace, by imposing a *left gauge-fixing condition*\n",
    "\n",
    "<center><img src=\"img/gaugeFix.svg\" alt=\"gauge fix\"></center>\n",
    "\n",
    "If we parametrize the tensor $B$ as\n",
    "\n",
    "<center><img src=\"img/VlX.svg\" alt=\"VlX\"></center>\n",
    "\n",
    "where $V_L$ is the $ D \\times d \\times D(d-1)$ tensor corresponding to the $D(d-1)$-dimensional null space of $A_L$ satisfying\n",
    "\n",
    "<center><img src=\"img/Vl.svg\" alt=\"Vl\"></center>\n",
    "\n",
    "then the gauge condition is automatically satisfied. In particular, this fixing of the gauge freedom ensures that the excitation is orthogonal to the ground state,\n",
    "\n",
    "<center><img src=\"img/excitationOrth.svg\" alt=\"excitationOrth\"></center>\n",
    "\n",
    "In this form, we have put forward an ansatz for an excited state characterized by a single $D(d-1) \\times D$ matrix $X$ such that\n",
    "\n",
    "1. All gauge degrees of freedom are fixed.\n",
    "2. All zero modes in the variational subspace are removed.\n",
    "3. Calculating the norm becomes straightforward.\n",
    "4. The excitation is orthogonal to the ground state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solving the eigenvalue problem\n",
    "\n",
    "Having introduced an excitation  ansatz which has all the right properties and is defined in terms of a single matrix $X$, all that is left to do is to minimize the energy function,\n",
    "\n",
    "$$  \\min_{X} \\frac{\\left \\langle \\Phi_p(X) \\middle | H  \\middle | \\Phi_p(X) \\right \\rangle}{\\left \\langle \\Phi_p(X) \\middle | \\Phi_p(X)  \\right \\rangle}. $$\n",
    "\n",
    "As both the numerator and the denominator are quadratic functions of the variational parameters $X$, this optimization problem reduces to solving a generalized eigenvalue problem\n",
    "\n",
    "$$ H_{\\text{eff}}(q) X = \\omega N_{\\text{eff}}(q) X, $$\n",
    "\n",
    "where the effective energy and normalization matrices are defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& 2\\pi\\delta(p-p') (\\boldsymbol{X'})^\\dagger H_{\\text{eff}}(q) \\boldsymbol{X} = \\left \\langle \\Phi_{p'}(X') \\middle | H  \\middle | \\Phi_p(X) \\right \\rangle \\\\\n",
    "& 2\\pi\\delta(p-p') (\\boldsymbol{X'})^\\dagger N_{\\text{eff}}(q) \\boldsymbol{X} = \\left \\langle \\Phi_{p'}(X') \\middle | \\Phi_p(X) \\right \\rangle,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and $\\boldsymbol{X}$ denotes a vectorized version of the matrix $X$. Since the overlap between two excited states is of the simple Euclidean form (cfr. the [lecture notes](https://doi.org/10.21468/SciPostPhysLectNotes.7)), the effective normalization matrix reduces to the unit matrix, and we are left with an ordinary eigenvalue problem.\n",
    "\n",
    "To solve this eigenvalue problem, we need to find an expression for $H_{\\text{eff}}$, or rather of the action thereof on a trial vector $\\boldsymbol{Y}$. In order to find this action we first transform the vector $\\boldsymbol{X}$ into a tensor $B$ by contracting its corresponding matrix with the right leg of $V_L$, and then compute all different contributions that pop up in a matrix element of the form $\\left \\langle \\Phi_p(B') \\middle | H  \\middle | \\Phi_p(B) \\right \\rangle$. This procedure is similar to what we have done when computing the gradient above, where we now need to take into account all different positions of the nearest-neighbor operator $h$ of the Hamiltonian, the input tensor $B$ and the output. Though slightly more involved than before, we can again define the following partion contractions\n",
    "\n",
    "<center><img src=\"img/LhMixed.svg\" alt=\"LhMixed\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"img/RhMixed.svg\" alt=\"RhMixed\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"img/LB.svg\" alt=\"LB\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"img/RB.svg\" alt=\"RB\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"img/L1.svg\" alt=\"L1\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"img/R1.svg\" alt=\"R1\"></center>\n",
    "\n",
    "Using these partial contractions, we find the action of the effective energy matrix on a given input tensor $B(Y)$ as\n",
    "\n",
    "<center><img src=\"img/HeffExcitation.svg\" alt=\"HeffExcitation\"></center>\n",
    "\n",
    "In the last step, we need the action of $H_{\\text{eff}}(p)$ on the vector $\\boldsymbol{Y}$, so we need to perform a last contraction\n",
    "\n",
    "<center><img src=\"img/quasi_inveff.svg\" alt=\"quasi_inveff\"></center>\n",
    "\n",
    "The total procedure is implemented in the routine `quasiParticle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function quasiParticle(h, Al, Ar, Ac, C, p, num; tol=1e-12)\n",
    "    # renormalize hamiltonian and find left and right environments\n",
    "    h̃ = reducedHamMixed(h, Ac, Ar)\n",
    "    Lh = LhMixed(h̃, Al, C; tol)\n",
    "    Rh = RhMixed(h̃, Ar, C; tol)\n",
    "    \n",
    "    function ApplyHeff(x)\n",
    "        # remember x is (D*(d-1), D))\n",
    "        \n",
    "        @tensor B[-1 -2; -3] := Vl[-1 -2; 1] * x[1; -3]\n",
    "        \n",
    "        function ApplyELR(x, p)\n",
    "            overlap = tr(C' * x)\n",
    "            @tensor y[-1; -2] := Al[-1 3; 1] * conj(Ar[-2; 3 2]) * x[1; 2]\n",
    "            y = x - exp(1im * p) * (y - overlap * C)\n",
    "            return y\n",
    "        end\n",
    "\n",
    "        function ApplyERL(x, p)\n",
    "            overlap = tr(C' * x)\n",
    "            @tensor y[-1; -2] := x[1; 2] * Ar[2; 3 -2] * conj(Al[1 3; -1])\n",
    "            y = x - exp(1im * p) * (y - overlap * C)\n",
    "            return y\n",
    "        end\n",
    "        \n",
    "        # right disconnected\n",
    "        @tensor right[-1; -2] := B[-1 2; 1] * conj(Ar[-2; 2 1])\n",
    "        right, _ = linsolve(v -> ApplyELR(v, p), right; tol)\n",
    "        \n",
    "        # left disconnected\n",
    "        @tensor left[-1; -2] := Lh[1; 2] * B[2 3; -2] * conj(Al[1 3; -1]) +\n",
    "                                Al[1 2; 4] * B[4 5; -2] * conj(Al[1 3; 6]) * conj(Al[6 7; -1]) * h̃[3 7; 2 5] +\n",
    "                                exp(-1im * p) * B[1 2; 4] * Ar[4 5; -2] * conj(Al[1 3; 6]) * conj(Al[6 7; -1]) * h̃[3 7; 2 5]\n",
    "        left, _ = linsolve(v -> ApplyERL(v, -p), left; tol)\n",
    "        \n",
    "        @tensor y[-1 -2; -3] := B[-1 2; 1] * Ar[1; 3 4] * conj(Ar[-3; 5 4]) * h̃[-2 5; 2 3] +\n",
    "                                exp(1im * p) * Al[-1 2; 1] * B[1 3; 4] * conj(Ar[-3; 5 4]) * h̃[-2 5; 2 3] +\n",
    "                                exp(-1im * p) * B[4 3; 1] * Ar[1; 2 -3] * conj(Al[4 5; -1]) * h̃[5 -2; 3 2] +\n",
    "                                Al[4 3; 1] * B[1 2; -3] * conj(Al[4 5; -1]) * h̃[5 -2; 3 2] +\n",
    "                                exp(1im * p) * Al[1 2; 4] * Al[4 5; 6] * conj(Al[1 3; -1]) * right[6; -3] * h̃[3 -2;2 5] +\n",
    "                                exp(2im * p) * Al[-1 6; 5] * Al[5 3; 2] * conj(Ar[-3; 4 1]) * right[2; 1] * h̃[-2 4; 6 3] +\n",
    "                                Lh[-1; 1] * B[1 -2; -3] +\n",
    "                                B[-1 -2; 1] * Rh[1; -3] +\n",
    "                                exp(-1im * p) * left[-1; 1] * Ar[1; -2 -3] +\n",
    "                                exp(+1im * p) * Lh[-1; 1] * Al[1 -2; 2] * right[2; -3]\n",
    "\n",
    "        @tensor yp[-1; -2] := y[1 2; -2] * conj(Vl[1 2; -1])\n",
    "        return yp\n",
    "    end\n",
    "\n",
    "    # find reduced parametrization\n",
    "    Vl = leftnull(Al, (1, 2), (3,));\n",
    "    \n",
    "    # solve eigenvalue problem\n",
    "    x0 = TensorMap(randn, ComplexF64, space(Vl, 3) ← space(Al, 1))\n",
    "    e, x = eigsolve(ApplyHeff, x0, num, :SR; tol)\n",
    "    \n",
    "    return x, e\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to compute the Haldane gap on top of the ground state of the spin-1 Heisenberg antiferromagnet we have just obtained using VUMPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pi\n",
    "num = 3\n",
    "x, e = quasiParticle(h, Al, Ar, Ac, C, p, num)\n",
    "@printf \"First triplet: %s\" join(real.(e), \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
